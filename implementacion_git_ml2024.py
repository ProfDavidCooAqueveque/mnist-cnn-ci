# -*- coding: utf-8 -*-
"""Implementacion_Git_ML2024.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kQk68ogL1mVhF5vlLZQerGSTcioRHTtt
"""

# MNIST Classification with CNN and Introduction to Continuous Integration

## Step 1: Import Required Libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.datasets import mnist
import numpy as np
import json

# Check TensorFlow Version
print("TensorFlow Version:", tf.__version__)

## Step 2: Load and Preprocess Dataset
# Load the MNIST dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize pixel values to the range [0, 1]
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add channel dimension for grayscale images
x_train = x_train[..., np.newaxis]
x_test = x_test[..., np.newaxis]

# Convert labels to one-hot encoding
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# Display the shapes of the datasets
print(f"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}")
print(f"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}")

## Step 3: Define the CNN Model
# Function to build the CNN
def build_cnn(input_shape=(28, 28, 1), num_classes=10):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D(pool_size=(2, 2)),
        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D(pool_size=(2, 2)),
        Flatten(),
        Dense(128, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    return model

# Create and compile the model
model = build_cnn()
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

## Step 4: Train the Model
# Train the model
history = model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print(f"Test accuracy: {test_acc}")

## Step 5: Save the Model and Training Metrics
# Save the model
model.save('mnist_cnn_model.h5')

# Save training metrics as a JSON file
with open('training_metrics.json', 'w') as f:
    json.dump(history.history, f)

print("Model and training metrics saved.")

## Step 6: Automated Tests
# Test: Verify the output shape of the model
def test_model_output_shape():
    test_model = build_cnn()
    output_shape = test_model.output_shape
    assert output_shape == (None, 10), f"Expected output shape: (None, 10), got: {output_shape}"

# Execute the test
try:
    test_model_output_shape()
    print("Test passed: Model output shape is correct.")
except AssertionError as e:
    print(f"Test failed: {e}")

# Test: Check minimum accuracy
MINIMUM_ACCURACY = 0.90
def test_minimum_accuracy():
    assert test_acc >= MINIMUM_ACCURACY, f"Test accuracy {test_acc} is below the required minimum {MINIMUM_ACCURACY}."

# Execute the test
try:
    test_minimum_accuracy()
    print("Test passed: Test accuracy meets the minimum requirement.")
except AssertionError as e:
    print(f"Test failed: {e}")

## Step 7: Explain Continuous Integration
print("""
Concept of Continuous Integration:
1. Every time you make changes to the code (e.g., model architecture or preprocessing),
   automated tests ensure that the changes do not break the functionality.
2. Tools like GitHub Actions can run these tests automatically every time you push code to the repository.
3. This ensures that the project maintains quality and meets the defined requirements.
""")